{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "CIFAR10_withtransformer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9AQW3dj8T27"
      },
      "source": [
        "# Image classification of CIFAR-10 dataset using vision transformer architecture.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lw5GX9Zc8T3A"
      },
      "source": [
        "## The entire code is divided into 6 sections:\n",
        "### 1. Importing the libraries\n",
        "### 2. Pre-processing and loading the dataset\n",
        "### 3. Building the model\n",
        "### 4. Training the model\n",
        "### 5. Visualizing the Output\n",
        "### 6. Prediction\n",
        "--------------------------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kH9TSZF8T3B",
        "outputId": "d1a12f9a-c54e-41f5-bb1b-fb59353b5744"
      },
      "source": [
        "# 1. Importing the Libraries\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import optim\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import os\n",
        "import socket\n",
        "\n",
        "#recognizing the CUDA device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEjVViGHg3Ex",
        "outputId": "1fef2fc7-90ab-4119-c391-5c70e5dda2cc"
      },
      "source": [
        "#setting up directory\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "os.chdir(\"/content/drive/MyDrive/studienarbeit\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f'GPU : {torch.cuda.get_device_name(device=device)}')\n",
        "print(f'CPUs: {torch.get_num_threads()}, GPUs: {torch.cuda.device_count()} on {socket.gethostname()}.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "GPU : Tesla T4\n",
            "CPUs: 1, GPUs: 1 on 7bb8dbf8a0bc.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2ltJpyz8T3E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66045353-0460-4203-fb02-72572008a4ee"
      },
      "source": [
        "# 2. Preprocessing and loading the data\n",
        "\n",
        "# preparatory to load dataset\n",
        "from six.moves import urllib    \n",
        "opener = urllib.request.build_opener()\n",
        "opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
        "urllib.request.install_opener(opener)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5) , (0.5, 0.5, 0.5)) ])\n",
        "trainset = torchvision.datasets.CIFAR10('./data', train = True, transform = transform, download = True)\n",
        "testset = torchvision.datasets.CIFAR10('./data',  transform = transform, download = True) \n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size = 128, shuffle = True)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size = 128, shuffle = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GetBGfK8T3E"
      },
      "source": [
        "#### Lets Visulise few images from a batch in dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXbOWT458T3E",
        "outputId": "9ebdf515-aaf2-4020-fc24-1cc3820a67a8"
      },
      "source": [
        "'''\n",
        "we can iterate over different batches in the data loader. it contains batches as lists. \n",
        "each batch is a list of two tensors. first tensor contains batch no. of images, second tensor contains\n",
        "batch no. of lables. \n",
        "'''\n",
        "batch = iter(trainloader).next()\n",
        "data = batch[0]\n",
        "labels = batch[1]\n",
        "print(type(batch))\n",
        "\n",
        "print(batch[0].shape, batch[1].shape)\n",
        "print(batch[0][1])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "torch.Size([128, 3, 32, 32]) torch.Size([128])\n",
            "tensor([[[ 0.0667, -0.3412, -0.1608,  ...,  0.0275, -0.1451,  0.0667],\n",
            "         [-0.0667, -0.4431, -0.1608,  ...,  0.1059,  0.2314,  0.2314],\n",
            "         [-0.3569, -0.5843, -0.5216,  ...,  0.1451,  0.1451,  0.1294],\n",
            "         ...,\n",
            "         [-0.5059, -0.5137, -0.4980,  ..., -0.3961, -0.3882, -0.4196],\n",
            "         [-0.4745, -0.4588, -0.4431,  ..., -0.5765, -0.5059, -0.5137],\n",
            "         [-0.5686, -0.5451, -0.5137,  ..., -0.6627, -0.6471, -0.6392]],\n",
            "\n",
            "        [[ 0.0039, -0.4980, -0.3725,  ..., -0.2392, -0.3098, -0.0667],\n",
            "         [-0.1765, -0.5137, -0.3020,  ..., -0.0431, -0.0431,  0.0588],\n",
            "         [-0.5216, -0.6627, -0.6392,  ..., -0.0275, -0.0745, -0.0667],\n",
            "         ...,\n",
            "         [-0.2235, -0.2314, -0.2157,  ..., -0.0667, -0.0588, -0.0902],\n",
            "         [-0.1922, -0.1529, -0.1451,  ..., -0.3647, -0.2392, -0.2314],\n",
            "         [-0.3098, -0.2549, -0.2157,  ..., -0.4902, -0.4588, -0.4588]],\n",
            "\n",
            "        [[-0.6314, -0.7412, -0.6863,  ..., -0.6863, -0.6941, -0.4510],\n",
            "         [-0.6784, -0.7569, -0.6471,  ..., -0.5765, -0.5922, -0.2863],\n",
            "         [-0.7333, -0.7333, -0.8196,  ..., -0.4902, -0.5686, -0.4667],\n",
            "         ...,\n",
            "         [ 0.4431,  0.4510,  0.4588,  ...,  0.6549,  0.6863,  0.6392],\n",
            "         [ 0.5059,  0.5765,  0.5686,  ...,  0.1765,  0.3804,  0.3961],\n",
            "         [ 0.3569,  0.4118,  0.4902,  ..., -0.0196,  0.0275,  0.0275]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKa_TNVB8T3G",
        "outputId": "ae31de91-2849-42f1-bc50-d26c562317b8"
      },
      "source": [
        "# model\n",
        "class PositionalEncoding(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, d_model = None,  patch_amount = None):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(patch_amount, d_model)\n",
        "        position = torch.arange(0, patch_amount, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = 10000.0**-(torch.arange(0,d_model,2)/(d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(1), :]\n",
        "        return x\n",
        "\n",
        "class Network(nn.Module):\n",
        "    \n",
        "    def __init__(self, patch_size=None, Image_size = None, channels = None,d_model=128, layers=1, nhead=32, dim_feedforward=256, dropout=0.15):\n",
        "        super().__init__()\n",
        "        \n",
        "        # patch to embedding\n",
        "        self.embedding = nn.Linear(patch_size*patch_size*channels,d_model)\n",
        "\n",
        "        # positional encoding ( use \"sine-cosine\" or \"learnable randomnumber\" positional encoding)      \n",
        "\n",
        "        patch_amount = int((Image_size/patch_size)**2)\n",
        "        #self.pos_enc = PositionalEncoding(d_model = d_model, patch_amount = patch_amount) # sine and cosine encoding\n",
        "        self.pos_enc = nn.Parameter(torch.rand(1,patch_amount,d_model)) # learnable position encoding\n",
        "        \n",
        "        # Transformer encoder block\n",
        "        layer_list = []\n",
        "        for layer in range(layers):\n",
        "          layer_list += [torch.nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout)]\n",
        "        self.encoder = torch.nn.Sequential(*layer_list)\n",
        "        # MLP on top of Transformer encoder block\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_model // 2, 10),\n",
        "        )\n",
        "                                \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = self.embedding(x)\n",
        "        #x = self.pos_enc(x)\n",
        "        x += self.pos_enc\n",
        "        x = self.encoder(x)   \n",
        "        x = self.classifier(x.max(dim=1)[0])\n",
        "        return x\n",
        "\n",
        "    def save_model(self, name):\n",
        "        torch.save(self.state_dict(), name)\n",
        "\n",
        "Image_size = 32 # image size\n",
        "patch_size = 8 # patch size\n",
        "channels = 3\n",
        "mymodel = Network(patch_size= patch_size, Image_size = Image_size, channels = channels, d_model = 512, layers= 5, nhead = 32, dim_feedforward = 256, dropout=0.15)\n",
        "print(mymodel)\n",
        "mymodel.to(device)\n",
        "criterion = nn.CrossEntropyLoss() # loss function\n",
        "print(f'Number of parameters: {sum(p.numel() for p in mymodel.parameters()):,}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Network(\n",
            "  (embedding): Linear(in_features=192, out_features=512, bias=True)\n",
            "  (encoder): Sequential(\n",
            "    (0): TransformerEncoderLayer(\n",
            "      (self_attn): MultiheadAttention(\n",
            "        (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (linear1): Linear(in_features=512, out_features=256, bias=True)\n",
            "      (dropout): Dropout(p=0.15, inplace=False)\n",
            "      (linear2): Linear(in_features=256, out_features=512, bias=True)\n",
            "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout1): Dropout(p=0.15, inplace=False)\n",
            "      (dropout2): Dropout(p=0.15, inplace=False)\n",
            "    )\n",
            "    (1): TransformerEncoderLayer(\n",
            "      (self_attn): MultiheadAttention(\n",
            "        (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (linear1): Linear(in_features=512, out_features=256, bias=True)\n",
            "      (dropout): Dropout(p=0.15, inplace=False)\n",
            "      (linear2): Linear(in_features=256, out_features=512, bias=True)\n",
            "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout1): Dropout(p=0.15, inplace=False)\n",
            "      (dropout2): Dropout(p=0.15, inplace=False)\n",
            "    )\n",
            "    (2): TransformerEncoderLayer(\n",
            "      (self_attn): MultiheadAttention(\n",
            "        (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (linear1): Linear(in_features=512, out_features=256, bias=True)\n",
            "      (dropout): Dropout(p=0.15, inplace=False)\n",
            "      (linear2): Linear(in_features=256, out_features=512, bias=True)\n",
            "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout1): Dropout(p=0.15, inplace=False)\n",
            "      (dropout2): Dropout(p=0.15, inplace=False)\n",
            "    )\n",
            "    (3): TransformerEncoderLayer(\n",
            "      (self_attn): MultiheadAttention(\n",
            "        (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (linear1): Linear(in_features=512, out_features=256, bias=True)\n",
            "      (dropout): Dropout(p=0.15, inplace=False)\n",
            "      (linear2): Linear(in_features=256, out_features=512, bias=True)\n",
            "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout1): Dropout(p=0.15, inplace=False)\n",
            "      (dropout2): Dropout(p=0.15, inplace=False)\n",
            "    )\n",
            "    (4): TransformerEncoderLayer(\n",
            "      (self_attn): MultiheadAttention(\n",
            "        (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (linear1): Linear(in_features=512, out_features=256, bias=True)\n",
            "      (dropout): Dropout(p=0.15, inplace=False)\n",
            "      (linear2): Linear(in_features=256, out_features=512, bias=True)\n",
            "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout1): Dropout(p=0.15, inplace=False)\n",
            "      (dropout2): Dropout(p=0.15, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=256, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "Number of parameters: 6,818,826\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nyMKqNq8T3H"
      },
      "source": [
        "def accuracy(outputs, labels):\n",
        "    _, preds = torch.max(outputs, dim=1)    \n",
        "    return torch.tensor(torch.sum(preds == labels).item() / len(preds)).type(torch.double)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzE7aOQjK1zk"
      },
      "source": [
        "#params = list(mymodel.parameters()) + list(pos.parameters())\n",
        "optimizer = optim.AdamW(mymodel.parameters(), lr = 0.0004, weight_decay = 0.005) #updating the parameters\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=300, gamma=0.85)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqZ7u3pW8T3H",
        "outputId": "ffbab070-e0ab-4abf-8e2f-a763e9aaf800"
      },
      "source": [
        "# Training the model\n",
        "epochs = 25\n",
        "print_interval = 150\n",
        "# iterating over the epochs\n",
        "for e in range(epochs):\n",
        "    running_loss = 0\n",
        "    output = None\n",
        "    acc = 0\n",
        "    # Iterating over the batches\n",
        "    for i,data in enumerate(trainloader, 0):        \n",
        "        # Preparing data\n",
        "        images, labels = data\n",
        "        images = images.to(device)   \n",
        "        patched_images = images.permute(0,2,3,1).unfold(1,patch_size,patch_size).unfold(2,patch_size,patch_size).reshape(images.shape[0],int((Image_size/patch_size)**2),-1)\n",
        "        # Transfering data on to GPU\n",
        "        labels = labels.to(device)\n",
        "       \n",
        "        # Set gradient to zeros\n",
        "        optimizer.zero_grad()       \n",
        "\n",
        "        #prediction, then optimizing the parameters\n",
        "        mymodel.train()\n",
        "        output = mymodel(patched_images)      \n",
        "        \n",
        "        loss = criterion(output, labels)        \n",
        "        loss.backward() # backward propagation\n",
        "        optimizer.step() # updates the parameters\n",
        "        scheduler.step()\n",
        "        \n",
        "        #accumulating loss and accuracy\n",
        "        running_loss += loss.item()\n",
        "        acc += accuracy(output, labels)\n",
        "        \n",
        "        #printing intermediate loss\n",
        "        if i % print_interval == (print_interval - 1):\n",
        "            print('[%d, %5d] loss: %.6f, accuracy:%4d ' % (e + 1, i + 1, running_loss / print_interval, (acc*100 / print_interval)))\n",
        "            running_loss = 0.0\n",
        "            acc = 0\n",
        "print(\"Model is trained\" )\n",
        "        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,   150] loss: 2.182387, accuracy:  17 \n",
            "[1,   300] loss: 1.909981, accuracy:  30 \n",
            "[2,   150] loss: 1.712071, accuracy:  37 \n",
            "[2,   300] loss: 1.630808, accuracy:  40 \n",
            "[3,   150] loss: 1.509915, accuracy:  45 \n",
            "[3,   300] loss: 1.455714, accuracy:  47 \n",
            "[4,   150] loss: 1.385370, accuracy:  49 \n",
            "[4,   300] loss: 1.347563, accuracy:  51 \n",
            "[5,   150] loss: 1.282300, accuracy:  53 \n",
            "[5,   300] loss: 1.278377, accuracy:  53 \n",
            "[6,   150] loss: 1.225641, accuracy:  56 \n",
            "[6,   300] loss: 1.207788, accuracy:  56 \n",
            "[7,   150] loss: 1.169333, accuracy:  57 \n",
            "[7,   300] loss: 1.157410, accuracy:  58 \n",
            "[8,   150] loss: 1.122341, accuracy:  59 \n",
            "[8,   300] loss: 1.121034, accuracy:  60 \n",
            "[9,   150] loss: 1.092940, accuracy:  60 \n",
            "[9,   300] loss: 1.077223, accuracy:  61 \n",
            "[10,   150] loss: 1.056177, accuracy:  62 \n",
            "[10,   300] loss: 1.057798, accuracy:  62 \n",
            "[11,   150] loss: 1.034599, accuracy:  63 \n",
            "[11,   300] loss: 1.030107, accuracy:  63 \n",
            "[12,   150] loss: 1.002792, accuracy:  64 \n",
            "[12,   300] loss: 1.010158, accuracy:  64 \n",
            "[13,   150] loss: 0.989666, accuracy:  64 \n",
            "[13,   300] loss: 0.996450, accuracy:  64 \n",
            "[14,   150] loss: 0.979264, accuracy:  65 \n",
            "[14,   300] loss: 0.981126, accuracy:  65 \n",
            "[15,   150] loss: 0.967062, accuracy:  65 \n",
            "[15,   300] loss: 0.961434, accuracy:  66 \n",
            "[16,   150] loss: 0.959899, accuracy:  66 \n",
            "[16,   300] loss: 0.959463, accuracy:  66 \n",
            "[17,   150] loss: 0.955622, accuracy:  66 \n",
            "[17,   300] loss: 0.944595, accuracy:  66 \n",
            "[18,   150] loss: 0.946427, accuracy:  66 \n",
            "[18,   300] loss: 0.948317, accuracy:  66 \n",
            "[19,   150] loss: 0.932501, accuracy:  66 \n",
            "[19,   300] loss: 0.948917, accuracy:  66 \n",
            "[20,   150] loss: 0.941300, accuracy:  66 \n",
            "[20,   300] loss: 0.938622, accuracy:  66 \n",
            "[21,   150] loss: 0.929511, accuracy:  66 \n",
            "[21,   300] loss: 0.928066, accuracy:  67 \n",
            "[22,   150] loss: 0.932892, accuracy:  67 \n",
            "[22,   300] loss: 0.932904, accuracy:  67 \n",
            "[23,   150] loss: 0.919012, accuracy:  67 \n",
            "[23,   300] loss: 0.936144, accuracy:  67 \n",
            "[24,   150] loss: 0.931501, accuracy:  67 \n",
            "[24,   300] loss: 0.920549, accuracy:  67 \n",
            "[25,   150] loss: 0.920943, accuracy:  67 \n",
            "[25,   300] loss: 0.925664, accuracy:  67 \n",
            "Model is trained\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mry_mDoB_OyZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93aaa13e-f9a5-45d9-92cd-6fbd2e72b4d3"
      },
      "source": [
        "#mymodel.save_model(\"cifar10_71percent\")\n",
        "mymodel.load_state_dict(torch.load(\"cifar10_71percent\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "s6F01zLwybSo",
        "outputId": "9755397c-89aa-483e-f56a-33dd3fdefeae"
      },
      "source": [
        "# Following is the Scree polt of the learnt positional encoding\n",
        "\n",
        "a = mymodel.pos_enc.squeeze()\n",
        "\n",
        "U, S, V = torch.svd(a)\n",
        "num_vars = 16\n",
        "eigvals = (S**2 / torch.sum(S**2)).cpu().detach().numpy()\n",
        "print(eigvals)\n",
        "fig = plt.figure(figsize= (8,5))\n",
        "sing_vals = np.arange(num_vars)+1\n",
        "plt.plot(sing_vals, eigvals, 'ro-', linewidth = 2)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.7708863  0.02012732 0.01867875 0.01814822 0.01764037 0.01676137\n",
            " 0.01647248 0.01605497 0.0154798  0.01538402 0.01388428 0.01325915\n",
            " 0.01302223 0.01196273 0.01156986 0.01066813]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "cDQ1PRX2wosv",
        "outputId": "8c280092-052f-41df-f0d3-52ac257e6511"
      },
      "source": [
        "# Visualizing the embedding weights\n",
        "\n",
        "img = mymodel.embedding.weight.detach().cpu().reshape(512,8,8,3)\n",
        "plt.imshow(img[25])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPe-gh0CgQte",
        "outputId": "b3f38a40-5a47-43e7-b02b-8230598cb526"
      },
      "source": [
        "#print(mymodel.pos_enc.max(dim=1))\n",
        "#print(mymodel.pos_enc[0,0,:]*1000)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[11,  6,  3,  0, 13,  7,  1,  5,  6, 13,  8,  6,  5,  8,  4, 10,  3, 14,\n",
              "          5,  5, 13, 10, 10, 15, 13,  1, 13,  2,  0,  8, 14, 15, 11, 15, 15,  5,\n",
              "         10, 13, 10,  3,  3,  4,  9,  0, 12, 14, 13,  0,  6, 12,  6, 15,  9, 13,\n",
              "         15,  1,  7,  1,  2,  9,  1,  4, 10, 14,  8,  4,  9,  1, 10,  9,  7,  7,\n",
              "          9,  8, 11, 14, 13, 12, 13, 15,  0,  0,  1, 14,  0,  1, 14,  5,  4,  9,\n",
              "          4,  1,  4,  0,  1, 13, 12, 15,  7,  3,  1,  4,  7,  2,  0,  7, 13, 11,\n",
              "         12,  6,  3, 10,  8,  4,  0,  3,  7, 10,  7,  3, 14,  6,  2,  7,  2,  7,\n",
              "          3, 10,  9,  3, 10,  8, 10,  3,  9,  5, 13, 12, 10,  5, 12,  3,  7,  0,\n",
              "          2,  6, 12, 10,  4,  9, 14,  0, 13,  0, 13, 11,  1,  0,  4, 15, 10, 15,\n",
              "          8,  6, 10,  2,  4, 12,  7,  2,  3,  2,  3,  4, 11,  2,  9, 11, 11,  2,\n",
              "         14,  8, 12,  0, 12, 11, 14,  6,  8,  8,  1, 13,  7, 12, 14,  4, 11,  9,\n",
              "         11,  5,  5,  0,  7, 13,  1,  3, 15,  1, 11, 10,  3,  8,  9, 14,  0,  2,\n",
              "          0,  9, 15, 11,  8,  0,  5,  9, 15,  5,  0,  9,  1,  1, 14, 10,  5,  8,\n",
              "          5,  1,  7, 11,  1, 13, 14, 12,  3, 11,  8, 11, 14,  0,  6,  2,  8, 14,\n",
              "         15, 13, 10,  8,  0, 14, 10, 13,  9, 15, 11,  3,  5, 10, 15,  4, 13,  4,\n",
              "         14, 10,  8, 10,  3,  2, 14, 12, 10, 10,  3,  1, 15,  1, 14, 11,  7, 13,\n",
              "          5,  9,  2,  9,  7, 11,  1,  6,  7, 15,  2,  4,  0,  1,  7,  3,  9,  7,\n",
              "          6, 10, 15,  3, 15, 13,  3,  7,  3,  9,  7,  0, 12,  1,  1,  5, 12,  2,\n",
              "          4, 10, 14,  2, 11,  2,  2,  4, 11,  3,  4,  2, 12,  6, 14,  8,  0, 12,\n",
              "          1,  3, 13,  4,  1, 14,  3,  7,  2,  9, 11,  3,  6, 11, 11,  9,  7, 13,\n",
              "          9,  5, 10,  6,  0, 14, 10,  6,  5,  3, 12,  9, 13,  1,  9, 10,  5,  1,\n",
              "          5, 12,  8, 13,  6,  5,  2,  4,  1,  9,  5, 12,  2,  7, 14,  8,  4,  1,\n",
              "         15,  0, 12, 13, 13, 15,  9,  3, 11,  0, 14, 14,  6,  7,  2,  7, 12, 13,\n",
              "          8, 14,  7, 14,  0, 10,  8, 15,  0,  6, 10,  4,  4,  2,  1,  9, 13, 14,\n",
              "         12,  5,  8, 14, 11, 11, 15,  8,  3, 10, 14,  9,  1, 14,  9, 10, 10,  6,\n",
              "         14,  8,  6, 13,  6, 12, 13, 15,  2, 15, 13,  7,  3,  3, 14, 11,  5,  2,\n",
              "         10,  8, 13,  4,  8, 10, 14,  2, 10,  5,  4, 13,  2, 12, 12,  4,  6, 12,\n",
              "          2,  8, 11,  0,  4,  3,  9,  9,  3, 12,  4,  8,  9,  3, 13, 14, 11, 12,\n",
              "          2,  9,  6,  3,  0,  0,  8,  0]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-NRt-Jb8T3I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7afb14d8-1d8f-4d90-9a20-6db1af033e46"
      },
      "source": [
        "# loading the testset\n",
        "testset = torchvision.datasets.CIFAR10('data/',  transform = transform, download = True)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size = 128, shuffle = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwLHMT1p8T3I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b49d930-e62c-4b3f-9def-eb3384811149"
      },
      "source": [
        "# mymodel.load_state_dict(torch.load(\"/content/drive/MyDrive/studienarbeit/mnist_99percent\"))\n",
        "# testing:\n",
        "running_loss_test = 0\n",
        "acc = 0\n",
        "mymodel.eval()\n",
        "with torch.no_grad(): # does this line matter? we are not optimizing anyway.\n",
        "  for images, labels in testloader:        \n",
        "      # pre-processing the images            \n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "      patched_images = images.permute(0,2,3,1).unfold(1,patch_size,patch_size).unfold(2,patch_size,patch_size).reshape(images.shape[0],int((Image_size/patch_size)**2),-1)\n",
        "      \n",
        "      # prediction\n",
        "      output = mymodel(patched_images) \n",
        "      acc += accuracy(output,labels) \n",
        "      loss = criterion(output, labels)            \n",
        "      running_loss_test += loss.item()\n",
        "\n",
        "\n",
        "print( \"testing loss: \", (running_loss_test/len(testloader)) )\n",
        "print( \"Test accuracy: %d\" % (acc*100/(len(testloader))) )\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "testing loss:  0.7179089762518168\n",
            "Test accuracy: 75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fReMM_yzUqD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}